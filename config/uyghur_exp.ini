[io]
# path to your input ints file
input_file = uyghur_data/uyghur_aug_11721_select_40.ints
# path to your output folder. this specifies the prefix of the output folder
output_dir = ./neural_exp/uyghur_22
# path to your dict file
dict_file = uyghur_data/uyghur_aug_11721_select_40.dict
# path to a global punct file. comment it out if there is none
punct_dict_file = uyghur_data/uyghur_punct.dict
# the elmo embeddings file
word_vecs_file = uyghur_data/uyghur_aug_11721_select_40.elmo.tch

[params]
# training iterations
iters = 30
# number of nonterminal categories
k = 50
# the depth of the grammar. always set it to -1.
d = -1
# initial alpha used by the Dirichlet initializer
init_alpha = 0.2
# maximum number per training batch. 4 is a good number for long sentences
max_num_per_batch = 4
# the file that has the gold tree annotation
gold_pcfg_file = uyghur_data/uyghur_all_40.linetrees
# this means how many sentences in the input are not in the gold. comment it out if all input sentences have gold trees
aug_number = 11721
# the name of the model
emission_type = flow-neuralexp
# type of flow architecture: nice or realnvp
flow_type = nice
# the learning rate of the nonterminal production model
lr_base = 1e-3
# the learning rate of the preterminal production model
lr_emit = 1e-3
# optimizer: Adam or Adam_split. If Adam is chosen, lr_emit has no effect
optimizer = Adam_split
# how loss is averaged. The default, sentence, is good.
batch_average = sentence
# how to initialize the models
init_method = dirichlet
# the number of batches processed between two evaluations.
num_batches_per_eval = 2000
# batch size of the evaluation using the Viterbi algorithm
viterbi_batch_size = 24
# the number of batches accumulated between two optimizer steps
num_batches_per_update = 1
# flag for evaluation before any updates
init_eval_flag = True
# the type of embedding: context or word
embedding_type = context
# flag for turning on the sim penalty loss
aux_losses = sim_penalty
# weight for the sim penalty loss
sim_penalty_scaler = 10
# the number of RNN layers in the nonterminal production model
num_rnn_layers = 2
# the number of flow blocks in the preterminal production model
num_flow_blocks = 4
